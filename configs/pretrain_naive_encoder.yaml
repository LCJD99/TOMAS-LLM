# NaiveEncoderForPretraining Training Configuration

# Model Configuration
model:
  llm_model_name: "Qwen2.5-7B"
  extended_tokenizer_path: "data/generated/extended_tokenizer"
  combined_tokens_path: "data/generated/combined_tokens.json"

# Data Configuration
data:
  train_data_path: "data/generated/naive_pretrain_data.jsonl"
  max_length: 512
  num_workers: 4
  
# Training Configuration
training:
  # Basic settings
  output_dir: "checkpoints/naive_encoder"
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 1
  num_train_epochs: 3
  
  # Optimization
  learning_rate: 5.0e-5
  weight_decay: 0.01
  warmup_steps: 100
  max_grad_norm: 1.0
  
  # Learning rate scheduler
  lr_scheduler_type: "cosine"
  
  # Logging
  logging_dir: "logs/naive_encoder"
  logging_steps: 10
  logging_first_step: true
  
  # Checkpointing
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 3
  
  # Evaluation (disabled by default)
  eval_strategy: "no"
  
  # Performance
  fp16: true  # Use mixed precision training
  dataloader_pin_memory: true
  dataloader_num_workers: 4
  
  # Other
  remove_unused_columns: false
  report_to: ["wandb"]  # Enable W&B logging
  seed: 42

# W&B Configuration
wandb:
  enabled: true
  project: "tomas-llm-naive-encoder"
  name: "naive_encoder_pretrain"
  tags:
    - "naive_encoder"
    - "vocabulary_extension"
    - "qwen2.5-7b"
  notes: "Pre-training NaiveEncoderForPretraining with extended vocabulary for tool configuration tokens"

# Paths
paths:
  checkpoint_dir: "checkpoints/naive_encoder"
  log_dir: "logs/naive_encoder"
  new_params_save_path: "checkpoints/naive_encoder/new_params_final.pt"

# Resume from checkpoint
resume:
  resume_from_checkpoint: null  # Set to "latest" or specific checkpoint path to resume
  load_new_params: true  # Whether to load previously saved new parameters
