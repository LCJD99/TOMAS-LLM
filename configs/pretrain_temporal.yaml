# Temporal Encoder Pretraining Configuration

# Model Configuration
model:
  llm_name: "Qwen/Qwen2.5-0.5B-Instruct"  # or "Qwen/Qwen2.5-7B-Instruct"
  llm_embedding_dim: 1024  # 0.5B: 1024, 7B: 3584
  freeze_llm: true
  
  temporal_encoder:
    hidden_channels: 64
    output_dim: 256
    num_layers: 3
    kernel_sizes: [3, 5, 7]  # Multi-scale convolution
    pooling: "adaptive_avg"  # "adaptive_avg", "adaptive_max", "flatten"
    normalization: "minmax"  # "minmax", "standard", "none"
    min_timesteps: 20
    max_timesteps: 100
    time_granularity_ms: 100

# Data Configuration
data:
  num_train_samples: 100000
  num_val_samples: 5000
  type_distribution:
    A: 0.4  # Trend description
    B: 0.3  # Bottleneck spotting
    C: 0.3  # Feasibility QA
  max_length: 256
  batch_size: 16
  num_workers: 4
  pin_memory: true
  
  # Data synthesis parameters
  noise_level: 0.05
  spike_probability: 0.3
  diversity_level: 0.8

# Training Configuration
training:
  num_epochs: 10
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_steps: 1000
  warmup_ratio: 0.1  # Alternative to warmup_steps
  gradient_clip: 1.0
  gradient_accumulation_steps: 1
  
  # Checkpointing
  save_interval: 1000  # Save every N steps
  eval_interval: 500   # Evaluate every N steps
  save_total_limit: 5  # Keep only last N checkpoints
  
  # Logging
  log_interval: 10  # Log every N steps
  
  # Mixed precision
  fp16: false
  bf16: false  # Use bfloat16 if available (recommended for A100)
  
  # Device
  device: "cuda"  # "cuda", "cpu", "mps"
  seed: 42

# Optimizer Configuration
optimizer:
  type: "AdamW"
  betas: [0.9, 0.999]
  eps: 1.0e-8
  
# Learning Rate Scheduler Configuration
scheduler:
  type: "cosine"  # "cosine", "linear", "constant", "constant_with_warmup"
  min_lr: 1.0e-6
  num_cycles: 0.5  # For cosine scheduler

# Paths
paths:
  output_dir: "checkpoints/temporal_pretrain"
  log_dir: "logs/temporal_pretrain"
  cache_dir: "data/temporal_pretrain/cache"
  tensorboard_dir: "logs/temporal_pretrain/tensorboard"

# Weights & Biases (optional)
wandb:
  enabled: false
  project: "temporal-encoder-pretrain"
  entity: null
  run_name: null  # Auto-generated if null
  tags: ["temporal", "pretrain", "qwen"]

# Evaluation Configuration
evaluation:
  metrics:
    - "perplexity"
    - "token_accuracy"
  num_samples: 1000  # Samples for detailed evaluation
  generate_samples: 20  # Number of samples to generate for qualitative check
