# TOMAS-LLM Encoder Pre-training Configuration

# ============================================================
# Data Configuration
# ============================================================
data:
  tool_registry: "data/tool_registry/tools.json"
  profiling_data: "data/profiling/profiling.csv"
  
  # Dataset augmentation settings
  augmentation:
    mode: "variation"  # "jitter", "variation", "both", "none"
    num_copies: 4  # Number of augmented copies per config
    jitter_ratio: 0.05  # Â±5% noise for numerical jittering
    use_variation: true  # Use template variations
    seed: 42  # Random seed for reproducibility

# ============================================================
# Model Configuration (REDESIGNED)
# ============================================================
model:
  # LLM model - can be either HuggingFace model ID or local path
  llm_model: "Qwen2.5-7B-Instruct"  # HF model ID or local path
  use_local_model: true # Set to true if llm_model is a local directory path
  local_model_path: null  # Alternative: specify local path here (overrides llm_model if set)
  
  # NEW: No need to specify llm_hidden_dim, num_tools - auto-detected
  # NEW: No freeze_semantic flag - always frozen by design
  
  d_resource: null  # null = auto-match LLM hidden dim (recommended)
  num_attention_heads: 8  # Gated fusion attention heads
  dropout: 0.1
  cache_dir: "hub"  # Only used when downloading from HuggingFace

# ============================================================
# Training Configuration
# ============================================================
training:
  # Basic settings
  batch_size: 8
  num_epochs: 100
  learning_rate: 1.0e-5
  weight_decay: 0.01
  
  # Convergence settings
  target_loss: 0.01  # Early stopping threshold (goal: overfit)
  
  # Optimization
  optimizer: "adamw"
  scheduler:
    type: "cosine"  # "cosine", "linear", "constant"
    eta_min_ratio: 0.1  # Minimum LR = lr * eta_min_ratio
    warmup_steps: 0  # Optional warmup steps
  
  # Gradient settings
  max_grad_norm: 1.0  # Gradient clipping
  gradient_accumulation_steps: 1  # Gradient accumulation
  
  # Mixed precision
  use_amp: false  # Automatic Mixed Precision (FP16)
  use_bf16: false  # BFloat16 training

# ============================================================
# Data Loading
# ============================================================
dataloader:
  num_workers: 4
  pin_memory: true  # Pin memory for faster GPU transfer
  shuffle: true
  drop_last: false
  prefetch_factor: 2  # Prefetch batches

# ============================================================
# Output Configuration
# ============================================================
output:
  output_dir: "assets"
  checkpoint_freq: 1  # Save checkpoint every N epochs
  save_best_only: false  # Also save periodic checkpoints
  
  # Checkpoint naming
  checkpoint_prefix: "encoder_checkpoint"
  best_model_name: "pretrained_encoder.pt"
  final_model_name: "pretrained_encoder_final.pt"

# ============================================================
# Logging Configuration
# ============================================================
logging:
  # Console logging
  log_level: "INFO"  # "DEBUG", "INFO", "WARNING", "ERROR"
  progress_bar: true
  
  # Weights & Biases
  wandb:
    enabled: false
    project: "tomas-encoder-pretrain"
    entity: null  # Your W&B username/team
    name: null  # Run name (auto-generated if null)
    tags: ["encoder", "pretrain", "phase0"]
    notes: "Resource encoder pre-training with self-supervised learning"
  
  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: "runs/pretrain_encoder"

# ============================================================
# Device Configuration
# ============================================================
device:
  type: "cuda"  # "cuda", "cpu", "mps"
  device_id: 0  # GPU device ID (if multiple GPUs)
  
# ============================================================
# Validation Configuration (Optional)
# ============================================================
validation:
  enabled: false  # Enable validation during training
  val_split: 0.1  # Validation split ratio
  val_freq: 5  # Validate every N epochs
  
# ============================================================
# Resume Configuration
# ============================================================
resume:
  enabled: false
  checkpoint_path: null  # Path to checkpoint to resume from
  resume_optimizer: true  # Resume optimizer state
  resume_scheduler: true  # Resume scheduler state

# ============================================================
# Advanced Settings
# ============================================================
advanced:
  # Tokenizer settings
  tokenizer:
    max_seq_length: 128  # Maximum sequence length for target text
    padding: "max_length"
    truncation: true
  
  # LLM settings for training
  llm:
    freeze_all: true  # Freeze all LLM parameters
    dtype: "float32"  # LLM dtype during training
    gradient_checkpointing: false  # Save memory (slower)
  
  # Encoder settings
  encoder:
    compile: false  # Use torch.compile (PyTorch 2.0+)
    
# ============================================================
# Experiment Tracking
# ============================================================
experiment:
  name: "encoder_pretrain_v1"
  description: "Pre-train resource encoder with self-supervised language modeling"
  tags: ["phase0", "encoder", "self-supervised"]
  
  # Reproducibility
  seed: 42
  deterministic: false  # Set to true for full reproducibility (slower)
