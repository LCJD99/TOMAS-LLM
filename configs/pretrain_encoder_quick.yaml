# TOMAS-LLM Encoder Pre-training - Quick Test Configuration
# For fast prototyping and testing

# ============================================================
# Data Configuration
# ============================================================
data:
  tool_registry: "data/tool_registry/tools.json"
  profiling_data: "data/profiling/profiling.csv"
  
  augmentation:
    mode: "jitter"  # Only jittering (faster)
    num_copies: 3  # 3x augmentation (faster training)
    jitter_ratio: 0.05
    use_variation: false
    seed: 42

# ============================================================
# Model Configuration
# ============================================================
model:
  llm_model: "Qwen/Qwen2.5-7B"
  llm_hidden_dim: 3584
  d_resource: 3584
  num_attention_heads: 8
  dropout: 0.1
  num_tools: 7
  freeze_semantic: true
  cache_dir: "hub"

# ============================================================
# Training Configuration
# ============================================================
training:
  batch_size: 64  # Larger batch for faster training
  num_epochs: 30  # Fewer epochs for quick test
  learning_rate: 2.0e-4  # Slightly higher LR
  weight_decay: 0.01
  target_loss: 0.05  # Less strict (faster convergence)
  
  optimizer: "adamw"
  scheduler:
    type: "cosine"
    eta_min_ratio: 0.1
    warmup_steps: 0
  
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
  use_amp: false
  use_bf16: false

# ============================================================
# Data Loading
# ============================================================
dataloader:
  num_workers: 2  # Fewer workers
  pin_memory: true
  shuffle: true
  drop_last: false
  prefetch_factor: 2

# ============================================================
# Output Configuration
# ============================================================
output:
  output_dir: "assets/test"  # Separate directory for tests
  checkpoint_freq: 5  # Save less frequently
  save_best_only: false
  
  checkpoint_prefix: "encoder_checkpoint"
  best_model_name: "pretrained_encoder.pt"
  final_model_name: "pretrained_encoder_final.pt"

# ============================================================
# Logging Configuration
# ============================================================
logging:
  log_level: "INFO"
  progress_bar: true
  
  wandb:
    enabled: false  # Disabled for quick tests
    project: "tomas-encoder-pretrain-test"
    entity: null
    name: null
    tags: ["test", "quick"]
    notes: "Quick test run"
  
  tensorboard:
    enabled: false
    log_dir: "runs/test"

# ============================================================
# Device Configuration
# ============================================================
device:
  type: "cuda"
  device_id: 0
  
# ============================================================
# Other Settings
# ============================================================
validation:
  enabled: false

resume:
  enabled: false
  checkpoint_path: null
  resume_optimizer: true
  resume_scheduler: true

advanced:
  tokenizer:
    max_seq_length: 128
    padding: "max_length"
    truncation: true
  
  llm:
    freeze_all: true
    dtype: "float32"
    gradient_checkpointing: false
  
  encoder:
    compile: false

experiment:
  name: "quick_test"
  description: "Quick test run for development"
  tags: ["test"]
  seed: 42
  deterministic: false
